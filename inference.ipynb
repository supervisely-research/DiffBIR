{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v1.10.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import os\n",
    "import math\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import einops\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.spaced_sampler import SpacedSampler\n",
    "from model.ddim_sampler import DDIMSampler\n",
    "from model.cldm import ControlLDM\n",
    "from utils.image import (\n",
    "    wavelet_reconstruction, adaptive_instance_normalization, auto_resize, pad\n",
    ")\n",
    "from utils.common import instantiate_from_config, load_state_dict\n",
    "from utils.file import list_image_files, get_file_name_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def process(\n",
    "    model: ControlLDM,\n",
    "    control_imgs: List[np.ndarray],\n",
    "    sampler: str,\n",
    "    steps: int,\n",
    "    strength: float,\n",
    "    color_fix_type: str,\n",
    "    disable_preprocess_model: bool,\n",
    "    positive_prompt: str = \"\",\n",
    "    prompt_scale: float = 1.0\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Apply DiffBIR model on a list of low-quality images.\n",
    "    \n",
    "    Args:\n",
    "        model (ControlLDM): Model.\n",
    "        control_imgs (List[np.ndarray]): A list of low-quality images (HWC, RGB, range in [0, 255])\n",
    "        sampler (str): Sampler name.\n",
    "        steps (int): Sampling steps.\n",
    "        strength (float): Control strength. Set to 1.0 during traning.\n",
    "        color_fix_type (str): Type of color correction for samples.\n",
    "        disable_preprocess_model (bool): If specified, preprocess model (SwinIR) will not be used.\n",
    "    \n",
    "    Returns:\n",
    "        preds (List[np.ndarray]): Restoration results (HWC, RGB, range in [0, 255]).\n",
    "        stage1_preds (List[np.ndarray]): Outputs of preprocess model (HWC, RGB, range in [0, 255]). \n",
    "            If `disable_preprocess_model` is specified, then preprocess model's outputs is the same \n",
    "            as low-quality inputs.\n",
    "    \"\"\"\n",
    "    n_samples = len(control_imgs)\n",
    "    if sampler == \"ddpm\":\n",
    "        sampler = SpacedSampler(model, var_type=\"fixed_small\")\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "    control = torch.tensor(np.stack(control_imgs) / 255.0, dtype=torch.float32, device=model.device).clamp_(0, 1)\n",
    "    control = einops.rearrange(control, \"n h w c -> n c h w\").contiguous()\n",
    "    # TODO: model.preprocess_model = lambda x: x\n",
    "    if not disable_preprocess_model and hasattr(model, \"preprocess_model\"):\n",
    "        control = model.preprocess_model(control)\n",
    "    elif disable_preprocess_model and not hasattr(model, \"preprocess_model\"):\n",
    "        raise ValueError(f\"model doesn't have a preprocess model.\")\n",
    "    \n",
    "    height, width = control.size(-2), control.size(-1)\n",
    "    cond = {\n",
    "        \"c_latent\": [model.apply_condition_encoder(control)],\n",
    "        \"c_crossattn\": [model.get_learned_conditioning([positive_prompt] * n_samples)]\n",
    "    }\n",
    "    model.control_scales = [strength] * 13\n",
    "    \n",
    "    shape = (n_samples, 4, height // 8, width // 8)\n",
    "    x_T = torch.randn(shape, device=model.device, dtype=torch.float32)\n",
    "    if isinstance(sampler, SpacedSampler):\n",
    "        samples = sampler.sample(\n",
    "            steps, shape, cond,\n",
    "            unconditional_guidance_scale=prompt_scale,\n",
    "            unconditional_conditioning=None,\n",
    "            cond_fn=None, x_T=x_T\n",
    "        )\n",
    "    else:\n",
    "        sampler: DDIMSampler\n",
    "        samples, _ = sampler.sample(\n",
    "            S=steps, batch_size=shape[0], shape=shape[1:],\n",
    "            conditioning=cond, unconditional_conditioning=None,\n",
    "            x_T=x_T, eta=0\n",
    "        )\n",
    "    x_samples = model.decode_first_stage(samples)\n",
    "    x_samples = ((x_samples + 1) / 2).clamp(0, 1)\n",
    "    \n",
    "    # apply color correction (borrowed from StableSR)\n",
    "    if color_fix_type == \"adain\":\n",
    "        x_samples = adaptive_instance_normalization(x_samples, control)\n",
    "    elif color_fix_type == \"wavelet\":\n",
    "        x_samples = wavelet_reconstruction(x_samples, control)\n",
    "    else:\n",
    "        assert color_fix_type == \"none\", f\"unexpected color fix type: {color_fix_type}\"\n",
    "    \n",
    "    x_samples = (einops.rearrange(x_samples, \"b c h w -> b h w c\") * 255).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "    control = (einops.rearrange(control, \"b c h w -> b h w c\") * 255).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    preds = [x_samples[i] for i in range(n_samples)]\n",
    "    stage1_preds = [control[i] for i in range(n_samples)]\n",
    "    \n",
    "    return preds, stage1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(x) -> Namespace:\n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"--ckpt\", required=True, type=str)\n",
    "    parser.add_argument(\"--config\", required=True, type=str)\n",
    "    parser.add_argument(\"--reload_swinir\", action=\"store_true\")\n",
    "    parser.add_argument(\"--swinir_ckpt\", type=str, default=\"\")\n",
    "    \n",
    "    parser.add_argument(\"--input\", type=str, required=True)\n",
    "    parser.add_argument(\"--sampler\", type=str, default=\"ddpm\", choices=[\"ddpm\", \"ddim\"])\n",
    "    parser.add_argument(\"--steps\", required=True, type=int)\n",
    "    parser.add_argument(\"--sr_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--image_size\", type=int, default=512)\n",
    "    parser.add_argument(\"--repeat_times\", type=int, default=1)\n",
    "    parser.add_argument(\"--disable_preprocess_model\", action=\"store_true\")\n",
    "    \n",
    "    parser.add_argument(\"--color_fix_type\", type=str, default=\"wavelet\", choices=[\"wavelet\", \"adain\", \"none\"])\n",
    "    parser.add_argument(\"--resize_back\", action=\"store_true\")\n",
    "    parser.add_argument(\"--output\", type=str, required=True)\n",
    "    parser.add_argument(\"--show_lq\", action=\"store_true\")\n",
    "    parser.add_argument(\"--skip_if_exist\", action=\"store_true\")\n",
    "    \n",
    "    parser.add_argument(\"--seed\", type=int, default=231)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\", choices=[\"cpu\", \"cuda\"])\n",
    "    \n",
    "    return parser.parse_args(x)\n",
    "\n",
    "args = parse_args(\"--ckpt weights/general_full_v1.ckpt --config configs/model/cldm.yaml --reload_swinir --swinir_ckpt weights/general_swinir_v1.ckpt --input X --output output --steps 50\".split(\" \"))\n",
    "args.input = \"inputs/my/\"\n",
    "args.output = \"output\"\n",
    "args.steps = 50\n",
    "args.sr_scale = 3\n",
    "args.image_size = 300\n",
    "args.resize_back = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "reload swinir model from weights/general_swinir_v1.ckpt\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model: ControlLDM = instantiate_from_config(OmegaConf.load(args.config))\n",
    "load_state_dict(model, torch.load(args.ckpt, map_location=\"cpu\"), strict=True)\n",
    "# reload preprocess model if specified\n",
    "if args.reload_swinir:\n",
    "    if not hasattr(model, \"preprocess_model\"):\n",
    "        raise ValueError(f\"model don't have a preprocess model.\")\n",
    "    print(f\"reload swinir model from {args.swinir_ckpt}\")\n",
    "    load_state_dict(model.preprocess_model, torch.load(args.swinir_ckpt, map_location=\"cpu\"), strict=True)\n",
    "model.freeze()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 50 steps using ddpm sampler\n",
      "start to sample from a given noise\n",
      "Running Spaced Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spaced Sampler: 100%|██████████| 50/50 [00:07<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to output/synthetic fruits (bbox)_ds0_182 (x300, sigma=4)_1.png\n",
      "start to sample from a given noise\n",
      "Running Spaced Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spaced Sampler: 100%|██████████| 50/50 [00:07<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to output/synthetic fruits (bbox)_ds0_182 (x300, sigma=2)_1.png\n"
     ]
    }
   ],
   "source": [
    "args.input = \"inputs/my/\"\n",
    "args.output = \"output\"\n",
    "args.steps = 50\n",
    "args.sr_scale = 3\n",
    "args.image_size = 300\n",
    "args.resize_back = True\n",
    "positive_prompt = \"\"\n",
    "prompt_scale = 1.\n",
    "strength = 0.95\n",
    "\n",
    "assert os.path.isdir(args.input)\n",
    "\n",
    "print(f\"sampling {args.steps} steps using ddpm sampler\")\n",
    "for file_path in list_image_files(args.input, follow_links=True):\n",
    "    lq = Image.open(file_path).convert(\"RGB\")\n",
    "    if args.sr_scale != 1:\n",
    "        lq = lq.resize(\n",
    "            tuple(math.ceil(x * args.sr_scale) for x in lq.size),\n",
    "            Image.BICUBIC\n",
    "        )\n",
    "    lq_resized = auto_resize(lq, args.image_size)\n",
    "    x = pad(np.array(lq_resized), scale=64)\n",
    "    \n",
    "    save_path = os.path.join(args.output, os.path.relpath(file_path, args.input))\n",
    "    parent_path, stem, _ = get_file_name_parts(save_path)\n",
    "    i = 0\n",
    "    save_path = os.path.join(parent_path, f\"{stem}_{i}.png\")\n",
    "    while os.path.exists(save_path):\n",
    "        i += 1\n",
    "        save_path = os.path.join(parent_path, f\"{stem}_{i}.png\")\n",
    "\n",
    "    \n",
    "    os.makedirs(parent_path, exist_ok=True)\n",
    "    \n",
    "    preds, stage1_preds = process(\n",
    "        model, [x], steps=args.steps, sampler=args.sampler,\n",
    "        strength=strength,\n",
    "        color_fix_type=args.color_fix_type,\n",
    "        disable_preprocess_model=args.disable_preprocess_model,\n",
    "        positive_prompt=positive_prompt,\n",
    "        prompt_scale=prompt_scale\n",
    "    )\n",
    "    \n",
    "    pred, stage1_pred = preds[0], stage1_preds[0]\n",
    "    \n",
    "    # remove padding\n",
    "    pred = pred[:lq_resized.height, :lq_resized.width, :]\n",
    "    stage1_pred = stage1_pred[:lq_resized.height, :lq_resized.width, :]\n",
    "    \n",
    "    if args.show_lq:\n",
    "        if args.resize_back:\n",
    "            if lq_resized.size != lq.size:\n",
    "                pred = np.array(Image.fromarray(pred).resize(lq.size, Image.LANCZOS))\n",
    "                stage1_pred = np.array(Image.fromarray(stage1_pred).resize(lq.size, Image.LANCZOS))\n",
    "            lq = np.array(lq)\n",
    "        else:\n",
    "            lq = np.array(lq_resized)\n",
    "        images = [lq, pred] if args.disable_preprocess_model else [lq, stage1_pred, pred]\n",
    "        Image.fromarray(np.concatenate(images, axis=1)).save(save_path)\n",
    "    else:\n",
    "        if args.resize_back and lq_resized.size != lq.size:\n",
    "            Image.fromarray(pred).resize(lq.size, Image.LANCZOS).save(save_path)\n",
    "        else:\n",
    "            Image.fromarray(pred).save(save_path)\n",
    "    print(f\"save to {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
